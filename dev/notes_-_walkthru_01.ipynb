{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on Fastai v2 Walk-thru 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Development using notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In a development notebook, any cell with a `#export` tag at the beginning will be exported a '.py' file that corresponds to the notebook.  The '.py' file is auto-generated from its notebook.  \n",
    "\n",
    "The code that is exported into the '.py' file should be the same as in the notebook, with the exception of `import` statements.  Something like `from local.torch_basics import *` becomes `from ..torch_basics import *` in the auto-generated '.py' file, in order to set the paths to the modules correctly.\n",
    "\n",
    "At the end of a development notebook, there is the cell:\n",
    "```python\n",
    "#hide\n",
    "from local.notebook.export import *\n",
    "notebook2script(all_fs=True)\n",
    "```\n",
    ".  When this is run, all the notebooks are converted into their '.py' files. \n",
    "\n",
    "The '.py' files are regular Python modules.  Whilst most of them are auto-generated, there are exceptions, with one being `core.imports`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests as documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tests in the development notebooks serve two purposes:\n",
    "\n",
    "1. They verify that the code is correct.\n",
    "2. They inform how the code can be used.\n",
    "\n",
    "Here is a way to run the tests in the notebooks from the terminal:\n",
    "```bash\n",
    "for i in {0,1,2}*.ipynb; do sleep 1; python run_notebook.py --fn $i & done\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pets Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was notebook 08 in the video.  At the time of writing, it's notebook 10.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local.test import *\n",
    "from local.data.all import *\n",
    "from local.vision.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = untar_data(URLs.PETS)/\"images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = get_image_files(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#7390) [/Users/jack/.fastai/data/oxford-iiit-pet/images/Egyptian_Mau_167.jpg,/Users/jack/.fastai/data/oxford-iiit-pet/images/pug_52.jpg,/Users/jack/.fastai/data/oxford-iiit-pet/images/basset_hound_112.jpg,/Users/jack/.fastai/data/oxford-iiit-pet/images/Siamese_193.jpg,/Users/jack/.fastai/data/oxford-iiit-pet/images/shiba_inu_122.jpg,/Users/jack/.fastai/data/oxford-iiit-pet/images/Siamese_53.jpg,/Users/jack/.fastai/data/oxford-iiit-pet/images/Birman_167.jpg,/Users/jack/.fastai/data/oxford-iiit-pet/images/leonberger_6.jpg,/Users/jack/.fastai/data/oxford-iiit-pet/images/Siamese_47.jpg,/Users/jack/.fastai/data/oxford-iiit-pet/images/shiba_inu_136.jpg...]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_idx = RandomSplitter()(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((#5912) [6320,1028,5912,4275,5888,1029,508,1484,4928,6537...],\n",
       " (#1478) [7112,1156,4299,5298,2499,3851,7001,532,5822,510...])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anything that returns something that can be called will have a name with the first letter capitalized.  Conventionally, in Python, only `class`es have the first letter capitalized.  But here, for example, `RandomSplitter` is a function, which returns a function.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `show()` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *type* with a `show` method is something that can be used by Fastai's transformation pipeline to show, or display, something.  For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitledImage(Tuple):\n",
    "    def show(self, ctx=None, **kwargs): show_titled_image(self, ctx=ctx, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is a type that knows how to show itself as a titled image.  Its `show` method can plot an image with a label in the title."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type annotations are not just for type-checking. They also add semantics to tensors, informing what kind of thing a tensor represents (an *image*, for example).  A same piece of code can also act differently and appropriately on different objects of different types.  For example, if something is of type `Image`, then, when displayed, it should show a picture of some sort, and because it is an image, it would make sense to apply a rotation transformation, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resized_image(fn:Path, sz=128):\n",
    "    x = Image.open(fn).convert('RGB').resize((sz,sz))\n",
    "    # Convert image to tensor for modeling\n",
    "    return tensor(array(x)).permute(2,0,1).float()/255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fn:Path` means that the first positional argument of `resized_image` is called `fn`, and it is expected to be of type `Path`.  Since `resized_image` is a normal Python function, `fn` doesn't really needs to be of type `Path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(resized_image(items[11]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(resized_image(str(items[11])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Transform`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things of type `Transform` are *transforms*.  In general, they have a `encodes` method, which converts the data into a form that is closer to being able to be used for modeling.  So, between the raw data and something that can be directly input into a model, there is one or more transforms.  \n",
    "\n",
    "Transforms tend to convert raw data into something that is less directly intelligible by us.  For example, we can look at an image and be able to see what things are in it, but not when it's been converted to a tensor, which is required for use by a model.  Therefore, transforms can also have a `decodes` method, which can convert data from a form that is not very intelligble by us to one that is more intelligible by us.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple example of a transform that converts a path of an image file to an image and its category label.  How it encodes is defined in the `encodes` method; how it decodes is defined in the `decodes` method.  After the transform is created, calling it calls the `encodes` method, while `.decode` calls the `decodes` method.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PetTfm(Transform):\n",
    "    def __init__(self, items, train_idx):\n",
    "        self.items,self.train_idx = items,train_idx\n",
    "        self.labeller = RegexLabeller(pat = r'/([^/]+)_\\d+.jpg$')\n",
    "        vals = map(self.labeller, items[train_idx])\n",
    "        self.vocab,self.o2i = uniqueify(vals, sort=True, bidir=True) \n",
    "\n",
    "    def encodes(self, i):\n",
    "        o = self.items[i]\n",
    "        return resized_image(o), self.o2i[self.labeller(o)]\n",
    "    \n",
    "    def decodes(self, x): return TitledImage(x[0],self.vocab[x[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pets = PetTfm(items, split_idx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = pets(0)\n",
    "x.shape,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = pets.decode((x,y))\n",
    "dec.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Transform` enforces type annotations.  Here is an example.  The function `resized_image` can be converted into a transform, so that the transform's `encodes` method implements it.  Now, the input to the transform *really* needs to be of the type `Path`, otherwise the transform does nothing and just returns the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transform: False (Path,object) -> resized_image "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Transform(resized_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Transform(resized_image)(items[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jack/.fastai/data/oxford-iiit-pet/images/basset_hound_112.jpg'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Transform(resized_image)(str(items[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way, it is possible to define a transform that can handle different inputs of different types. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `uniqueify`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `uniqueify` function takes a list of values and turn it into a vocabulary, a list of the unique values of the given list.  It also has a `bidir` keyword argument, which, when set to `True`, will also return the reverse mapping, from index to object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab,o2i = uniqueify(vals, sort=True, bidir=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `L`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is Fastai's own sequence type that's meant to be analogous to Python's `list`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = L(5, 6, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#3) [5,6,8]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#3) [-5,-6,-8]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.map(operator.neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((#4) [5,6,8,0], (#4) [5,5,6,8])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + 0, 5 + a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#3) [5,9,100]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L([5, 'h'], [9, 'j'], [100, 'k']).itemgot(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Pipeline`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Pipeline` can be used to compose a sequence of transforms, so they will be applied one after another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `TupleTransform`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TupleTransform` is a special type of transform, which, when given a tuple to encode, will apply its `encodes` method to each element in the tuple.  This is convenient because things tend to be grouped in a tuple along a transform pipeline, so instead of having to apply a transform to each element inside a loop, just use `TupleTransform`.  Note that it has to be tuples, not list, because Pytorch's dataloader only works with tuples. \n",
    "\n",
    "Note that the only difference between `Transform` and `TupleTransform` is that `TupleTransform` has its attribute `as_item_force` set to `False`.  This attribute specifies whether the transform type should apply its `encodes` method to the input as a whole, or to each element of the input.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TupleTransform??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `DataSource`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DataSource` applies a transform, or transform pipeline, to a list of items.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageResizer(Transform):\n",
    "    order=10\n",
    "    \"Resize image to `size` using `resample`\"\n",
    "    def __init__(self, size, resample=Image.BILINEAR):\n",
    "        if not is_listy(size): size=(size,size)\n",
    "        self.size,self.resample = (size[1],size[0]),resample\n",
    "\n",
    "    def encodes(self, o:PILImage): return o.resize(size=self.size, resample=self.resample)\n",
    "    def encodes(self, o:PILMask):  return o.resize(size=self.size, resample=Image.NEAREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [[PILImage.create, ImageResizer(128), ToTensor(), IntToFloatTensor()],\n",
    "        [labeller, Categorize()]]\n",
    "dsrc = DataSource(items, tfms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its two main input arguments are `items`,  a list of items, and `tfms`, a list of list of transforms, .  In general, `tfms` is of length 2, with the first element being the list of transforms for the independent variable, the second element being the list of transforms for the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type Dispatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ImageResizer` above, when given a `PILImage`, will resample using `Image.BILINEAR` scheme.  When given a `PILMask`, it will resample using `Image.NEAREST` scheme.  This is called *type dispatch*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, most types in Fastai has a `create` class method that can be used to create an instance of that type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
